{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DATA EXTRACTION MODULE"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T08:42:48.549356Z",
     "start_time": "2024-07-31T08:42:48.345929Z"
    }
   },
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import timedelta\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from loguru import logger\n",
    "from pandasdmx import Request\n",
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import pandasdmx\n",
    "import pandasdmx as sdmx\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECB | ESTâ‚¬R\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pandasdmx import Request\n",
    "\n",
    "logger.info(\"Starting to retrieve data from ECB SDMX API\")\n",
    "logger.info(\"Dataset is EST - Euro Short Term Rate\")\n",
    "\n",
    "# Define the data URL\n",
    "url = \"https://sdw-wsrest.ecb.europa.eu/service/data/\"\n",
    "\n",
    "# Define the dataset IDs\n",
    "datasets = [\"ECB\", \"EST\", \"1.0\"]\n",
    "\n",
    "# Define the dimensions\n",
    "dimensions = \"all\"\n",
    "\n",
    "# Define the time period\n",
    "start_period = \"2020-01-01\"\n",
    "end_period = \"2024-06-30\"\n",
    "\n",
    "logger.info(\"Retrieving data for the period: {} - {}\", start_period, end_period)\n",
    "\n",
    "# Construct the full URL\n",
    "full_url = f\"{url}{','.join(datasets)}/{dimensions}/?startPeriod={start_period}&endPeriod={end_period}\"\n",
    "logger.info(\"Starting to retrieve data from: {}\", full_url)\n",
    "\n",
    "# Make the request\n",
    "r = requests.get(full_url)\n",
    "\n",
    "logger.info(\"Request completed with status code: {}\", r.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if r.status_code == 200:\n",
    "    # Parse the response using pandas_sdmx\n",
    "    data = Request(\"ECB\", log_level=0).get(url=full_url)\n",
    "\n",
    "    logger.info(\"Data retrieved successfully\")\n",
    "    \n",
    "    # Convert data to DataFrame\n",
    "    df = data.to_pandas()\n",
    "\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "else:\n",
    "    logger.error(\"Failed to retrieve data\")\n",
    "    logger.error(\"Status code: {}\", r.status_code)\n",
    "\n",
    "df.to_csv(f'FETCHED_DATA/ECB_API/EST.csv')\n",
    "logger.info(\"Data saved to file: {}\", f'EST.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ECB | CISS "
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the data URL\n",
    "logger.info(\"Starting to retrieve data from ECB SDMX API\")\n",
    "logger.info(\"Dataset is CISS - Composite Indicator of Systemic Stress\")\n",
    "\n",
    "url = \"https://sdw-wsrest.ecb.europa.eu/service/data/\"\n",
    "\n",
    "# Define the dataset IDs\n",
    "datasets = [\"ECB\", \"CISS\", \"1.0\"]\n",
    "\n",
    "# Define the dimensions\n",
    "dimensions = \"all\"\n",
    "\n",
    "# Define the time period\n",
    "start_period = \"2020-01-01\"\n",
    "end_period = \"2024-06-30\"\n",
    "\n",
    "logger.info(\"Retrieving data for the period: {} - {}\", start_period, end_period)\n",
    "\n",
    "# Construct the full URL\n",
    "full_url = f\"{url}{','.join(datasets)}/{dimensions}/?startPeriod={start_period}&endPeriod={end_period}\"\n",
    "logger.info(\"Starting to retrieve data from: {}\", full_url)\n",
    "\n",
    "# Make the request\n",
    "r = requests.get(full_url)\n",
    "\n",
    "logger.info(\"Request completed with status code: {}\", r.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if r.status_code == 200:\n",
    "    # Parse the response using pandas_sdmx\n",
    "    data = Request(\"ECB\", log_level=0).get(url=full_url)\n",
    "\n",
    "    logger.info(\"Data retrieved successfully\")\n",
    "    \n",
    "    # Convert data to DataFrame\n",
    "    df = data.to_pandas()\n",
    "\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "else:\n",
    "    logger.error(\"Failed to retrieve data\")\n",
    "    logger.error(\"Status code: {}\", r.status_code)\n",
    "df.to_csv(f'FETCHED_DATA/ECB_API/ECB_CISS.csv')\n",
    "logger.info(\"Data saved to file: {}\", f'ECB_CISS.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ECB | EXCHANGE RATES USD|CNY\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import pandasdmx as sdmx\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch exchange rates from ECB SDMX API\n",
    "def fetch_exchange_rates(start_date, end_date, base_currency, target_currencies):\n",
    "   \n",
    "\n",
    "    # Define the dataset IDs\n",
    "    datasets = [\"EXR\", \"D\", \"+\".join(target_currencies), base_currency, \"SP00\", \"A\"]\n",
    "\n",
    "    # Define the dimensions\n",
    "\n",
    "    # Construct the full URL\n",
    "    full_url = f\"{url}{datasets[0]}/{'.'.join(datasets[1:])}?\" + f\"startPeriod={start_date}&endPeriod={end_date}\"\n",
    "\n",
    "    logger.info(\"Starting to retrieve data from: {}\", full_url)\n",
    "\n",
    "    \n",
    "    response = requests.get(full_url)\n",
    "    if response.status_code != 200:\n",
    "        logger.error(\"Failed to fetch data from ECB SDMX API\")\n",
    "        logger.error(\"Status code: {}\", response.status_code)\n",
    "        return None\t\n",
    "    logger.info(\"Data fetched successfully\")\n",
    "    data = Request(\"ECB\", log_level=0).get(url=full_url)\n",
    "    logger.info(\"Data retrieved successfully\")    \n",
    "    # Convert data to DataFrame\n",
    "    df = data.to_pandas()\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "    return df\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    data.to_csv(filename, index=True)\n",
    "    logger.info(\"Data saved to {}\", filename)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Main function to encapsulate the process\n",
    "def main(start_date, end_date, base_currency, target_currencies, output_file):\n",
    "    raw_data = fetch_exchange_rates(start_date, end_date, base_currency, target_currencies)\n",
    "    logger.info(\"Data fetched successfully\")\n",
    "    save_to_csv(raw_data, output_file)\n",
    "\n",
    "# Parameters\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-06-30'\n",
    "base_currency = 'EUR'\n",
    "target_currencies = ['USD', 'CNY'] # add more currencies if needed\n",
    "output_file = f'FETCHED_DATA/ECB_API/EXCHANGE_RATES_{base_currency}_to_{target_currencies}.csv'\n",
    "\n",
    "# Execute the main function\n",
    "main(start_date, end_date, base_currency, target_currencies, output_file)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ESTAT | TEC00118"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandasdmx\n",
    "\n",
    "# Define the Eurostat API URL\n",
    "\n",
    "url_eurostat = \"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/tec00118\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from Eurostat SDMX API\")\n",
    "logger.info(\"Dataset is TEC00118 - Harmonised indices of consumer prices - inflation rate\")\n",
    "\n",
    "# Define the time period\n",
    "start_period_eurostat = \"2020-01\"\n",
    "end_period_eurostat = \"2024-06\"\n",
    "\n",
    "logger.info(\"Retrieving data for the period: {}\", start_period_eurostat)\n",
    "\n",
    "# Construct the full URL\n",
    "full_url_eurostat = f\"{url_eurostat}?startPeriod={start_period_eurostat}&endPeriod={end_period_eurostat}\"\n",
    "logger.info(\"Starting to retrieve data from: {}\", full_url_eurostat)\n",
    "# Make the request\n",
    "r_eurostat = requests.get(full_url_eurostat)\n",
    "\n",
    "logger.info(\"Request completed with status code: {}\", r_eurostat.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if r_eurostat.status_code == 200:\n",
    "    # Load the data into a DataFrame\n",
    "    # Make the request and parse the response using pandas_sdmx\n",
    "    data_eurostat = pandasdmx.Request(\"ESTAT\").get(url=full_url_eurostat)\n",
    "\n",
    "    logger.info(\"Data retrieved successfully\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df_eurostat = data_eurostat.to_pandas()\n",
    "\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "else:\n",
    "    logger.error(\"Failed to retrieve data\")\n",
    "    logger.error(\"Status code: {}\", r_eurostat.status_code)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_eurostat.to_csv(f'FETCHED_DATA/ESTAT_API/ESTAT_INFLATION.csv')\n",
    "logger.info(\"Data saved to file: {}\", f'ESTAT_INFLATION.csv')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ESTAT | TEC00115"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the Eurostat API URL\n",
    "url_eurostat = \"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/tec00115\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from Eurostat SDMX API\")\n",
    "logger.info(\"Dataset is TEC00115 - Gross domestic product at market prices\")\n",
    "\n",
    "# Define the time period\n",
    "start_period_eurostat = \"2020-01\"\n",
    "end_period_eurostat = \"2024-06\"\n",
    "\n",
    "\n",
    "logger.info(\"Retrieving data for the period: {}\", start_period_eurostat)\n",
    "\n",
    "# Construct the full URL\n",
    "full_url_eurostat = f\"{url_eurostat}?startPeriod={start_period_eurostat}&endPeriod={end_period_eurostat}\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from: {}\", full_url_eurostat)\n",
    "\n",
    "# Make the request\n",
    "r_eurostat = requests.get(full_url_eurostat)\n",
    "\n",
    "logger.info(\"Request completed with status code: {}\", r_eurostat.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if r_eurostat.status_code == 200:\n",
    "    # Load the data into a DataFrame\n",
    "    # Make the request and parse the response using pandas_sdmx\n",
    "    data_eurostat = pandasdmx.Request(\"ESTAT\").get(url=full_url_eurostat)\n",
    "\n",
    "    logger.info(\"Data retrieved successfully\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df_eurostat = data_eurostat.to_pandas()\n",
    "\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "else:\n",
    "    logger.error(\"Failed to retrieve data\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_eurostat.to_csv(f'FETCHED_DATA/ESTAT_API/ESTAT_GDP.csv')\n",
    "logger.info(\"Data saved to file: {}\", f'ESTAT_GDP.csv')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ESTAT | EI_LMHR_M "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandasdmx\n",
    "\n",
    "# Define the Eurostat API URL\n",
    "url_eurostat = \"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/ei_lmhr_m\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from Eurostat SDMX API\")\n",
    "logger.info(\"Dataset is EI_LMHR_M - Unemployment rate\")\n",
    "\n",
    "# Define the time period\n",
    "start_period_eurostat = \"2020-01\"\n",
    "end_period_eurostat = \"2024-07\"\n",
    "\n",
    "logger.info(\"Retrieving data for the period: {}\", start_period_eurostat)\n",
    "\n",
    "# Construct the full URL\n",
    "full_url_eurostat = f\"{url_eurostat}?startPeriod={start_period_eurostat}&endPeriod={end_period_eurostat}\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from: {}\", full_url_eurostat)\n",
    "\n",
    "# Make the request\n",
    "r_eurostat = requests.get(full_url_eurostat)\n",
    "\n",
    "logger.info(\"Request completed with status code: {}\", r_eurostat.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if r_eurostat.status_code == 200:\n",
    "    # Load the data into a DataFrame\n",
    "    # Make the request and parse the response using pandas_sdmx\n",
    "    data_eurostat = pandasdmx.Request(\"ESTAT\").get(url=full_url_eurostat)\n",
    "\n",
    "    logger.info(\"Data retrieved successfully\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df_eurostat = data_eurostat.to_pandas()\n",
    "\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "else:\n",
    "    logger.error(\"Failed to retrieve data\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_eurostat.to_csv(f'FETCHED_DATA/ESTAT_API/ESTAT_UNEMPLOYMENT.csv')\n",
    "logger.info(\"Data saved to file: {}\", f'ESTAT_UNEMPLOYMENT.csv')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ESTAT | TEIMF050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the Eurostat API URL\n",
    "url_eurostat = \"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/teimf050\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from Eurostat SDMX API\")\n",
    "\n",
    "# Define the time period\n",
    "start_period_eurostat = \"2020-01\"\n",
    "end_period_eurostat = \"2024-06\"\n",
    "\n",
    "\n",
    "logger.info(\"Retrieving data for the period: {}\", start_period_eurostat)\n",
    "\n",
    "# Construct the full URL\n",
    "full_url_eurostat = f\"{url_eurostat}?startPeriod={start_period_eurostat}&endPeriod={end_period_eurostat}\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from: {}\", full_url_eurostat)\n",
    "\n",
    "# Make the request\n",
    "r_eurostat = requests.get(full_url_eurostat)\n",
    "\n",
    "logger.info(\"Request completed with status code: {}\", r_eurostat.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if r_eurostat.status_code == 200:\n",
    "    # Load the data into a DataFrame\n",
    "    # Make the request and parse the response using pandas_sdmx\n",
    "    data_eurostat = pandasdmx.Request(\"ESTAT\").get(url=full_url_eurostat)\n",
    "\n",
    "    logger.info(\"Data retrieved successfully\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df_eurostat = data_eurostat.to_pandas()\n",
    "\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "else:\n",
    "    logger.error(\"Failed to retrieve data\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_eurostat.to_csv(f'FETCHED_DATA/ESTAT_API/ESTAT_GOVYC.csv')\n",
    "logger.info(\"Data saved to file: {}\", f'ESTAT_GOVYC.csv')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ESTAT | EI_BSCO_M"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Define the Eurostat API URL\n",
    "url_eurostat = \"https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/ei_bsco_m\"\n",
    "\n",
    "logger.info(\"Starting to retrieve data from Eurostat SDMX API\")\n",
    "logger.info(\"Dataset is ei_bsco_m - Consumer Confidence\")\n",
    "\n",
    "# Define the time period\n",
    "start_period_eurostat = \"2020-01\"\n",
    "end_period_eurostat = \"2024-06\"\n",
    "\n",
    "# Construct the full URL\n",
    "full_url_eurostat = f\"{url_eurostat}?startPeriod={start_period_eurostat}&endPeriod={end_period_eurostat}\"\n",
    "\n",
    "# Make the request\n",
    "r_eurostat = requests.get(url_eurostat)\n",
    "\n",
    "logger.info(\"Request completed with status code: {}\", r_eurostat.status_code)\n",
    "\n",
    "# Check if the request was successful\n",
    "if r_eurostat.status_code == 200:\n",
    "    # Load the data into a DataFrame\n",
    "    # Make the request and parse the response using pandas_sdmx\n",
    "    data_eurostat = pandasdmx.Request(\"ESTAT\").get(url=full_url_eurostat)\n",
    "\n",
    "    logger.info(\"Data retrieved successfully\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df_eurostat = data_eurostat.to_pandas()\n",
    "\n",
    "    logger.info(\"Data converted to DataFrame\")\n",
    "else:\n",
    "    logger.error(\"Failed to retrieve data\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_eurostat.to_csv(f'FETCHED_DATA/ESTAT_API/ESTAT_CONSUMER_CONFIDENCE.csv')\n",
    "logger.info(\"Data saved to file: {}\", f'ESTAT_CONSUMER_CONFIDENCE.csv')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## WHO | COVID-19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# URL for the WHO CSV file\n",
    "url = \"https://srhdpeuwpubsa.blob.core.windows.net/whdh/COVID/WHO-COVID-19-global-data.csv\"\n",
    "\n",
    "# Fetch the CSV file directly from the URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Save the CSV file locally\n",
    "with open('WHO-COVID-19-global-data.csv', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv('WHO-COVID-19-global-data.csv')\n",
    "\n",
    "# Filter data for specific countries and relevant columns\n",
    "countries = ['Austria', 'Belgium', 'Cyprus', 'Estonia', 'Finland', 'France', 'Germany',\n",
    "    'Greece', 'Ireland', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Malta',\n",
    "    'Netherlands', 'Portugal', 'Slovakia', 'Slovenia', 'Spain', 'Croatia']\n",
    "filtered_data = data[data['Country'].isin(countries)][['Date_reported', 'Country', 'New_cases', 'Cumulative_cases', 'New_deaths', 'Cumulative_deaths']]\n",
    "\n",
    "# Replace NaN values with zeros\n",
    "filtered_data = filtered_data.fillna(0)\n",
    "\n",
    "# Ensure all relevant columns are numeric\n",
    "numeric_columns = ['New_cases', 'Cumulative_cases', 'New_deaths', 'Cumulative_deaths']\n",
    "for column in numeric_columns:\n",
    "    filtered_data[column] = pd.to_numeric(filtered_data[column], errors='coerce')\n",
    "\n",
    "# Convert the 'Date_reported' column to datetime format\n",
    "filtered_data['Date_reported'] = pd.to_datetime(filtered_data['Date_reported'], errors='coerce')\n",
    "\n",
    "# Filter the data to keep only dates up to 30/06/2024\n",
    "filtered_data = filtered_data[filtered_data['Date_reported'] <= '2024-06-30']\n",
    "\n",
    "# Group by Date and calculate the sum for numeric columns only\n",
    "summed_data = filtered_data.groupby('Date_reported')[numeric_columns].sum().reset_index()\n",
    "\n",
    "# Add the 'Country' column with the value 'EA20'\n",
    "summed_data['Country'] = 'EA20'\n",
    "\n",
    "# Reorder columns to have 'Date_reported' and 'Country' first\n",
    "summed_data = summed_data[['Date_reported', 'Country'] + numeric_columns]\n",
    "\n",
    "# Save the summed data to a CSV file\n",
    "summed_data.to_csv('FETCHED_DATA/WHO/COVID_DATA_EA20.csv', index=False)\n",
    "print(f\"Summed data saved to FETCHED_DATA/WHO_API/COVID_DATA_EA20.csv\")\n",
    "\n",
    "# Display the first few rows of the summed data for verification\n",
    "summed_data.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ALPHA VANTAGE | COMMODITIES"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your API key\n",
    "api_key = '-'\n",
    "\n",
    "# Initialize the TimeSeries class\n",
    "ts = TimeSeries(key=api_key, output_format='pandas')\n",
    "\n",
    "# Define the commodity symbols (using ETFs where necessary)\n",
    "commodities = {\n",
    "    'oil': 'BNO',       # United States Brent Oil Fund\n",
    "    'gas': 'UNG',       # United States Natural Gas Fund\n",
    "    'corn': 'CORN',     # Teucrium Corn Fund\n",
    "    'wheat': 'WEAT'     # Teucrium Wheat Fund\n",
    "}\n",
    "\n",
    "# Fetch data from Alpha Vantage\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-06-30'\n",
    "data = {}\n",
    "\n",
    "for commodity, symbol in commodities.items():\n",
    "    df, _ = ts.get_daily(symbol=symbol, outputsize='full')\n",
    "    df.index = pd.to_datetime(df.index)  # Ensure the index is datetime\n",
    "    df = df.sort_index()  # Sort the index\n",
    "    df = df.loc[start_date:end_date]  # Filter the date range\n",
    "    data[commodity] = df\n",
    "\n",
    "# Display the data\n",
    "for commodity, df in data.items():\n",
    "    print(f\"\\n{commodity.capitalize()} Prices:\\n\")\n",
    "    print(df.head())\n",
    "\n",
    "# Optionally, save the data to CSV files\n",
    "for commodity, df in data.items():\n",
    "    df.to_csv(f'FETCHED_DATA/ALPHA_VANTAGE_API/{commodity}_prices.csv')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PYTRENDS | KEY-PHRASES"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "keywords = ['\"Russia Ukraine War\"', '\"Israel Gaza War\"', '\"Financial Crisis\"', '\"Climate Change\"', '\"Covid-19\"']\n",
    "timeframe = '2020-01-01 2024-06-30'\n",
    "\n",
    "pytrends.build_payload(kw_list=keywords, cat=0, timeframe=timeframe, geo='', gprop='')\n",
    "\n",
    "interest_over_time_df = pytrends.interest_over_time()\n",
    "\n",
    "if not interest_over_time_df.empty:\n",
    "    interest_over_time_df = interest_over_time_df.drop(columns=['isPartial'])\n",
    "time.sleep(60)\n",
    "\n",
    "print(interest_over_time_df)\n",
    "\n",
    "interest_over_time_df.to_csv('trends_data_over.csv')\n",
    "\n",
    "\n",
    "def convert_weekly_to_daily(weekly_df):\n",
    "    daily_data = []\n",
    "    for i in range(len(weekly_df) - 1):\n",
    "        week_data = weekly_df.iloc[i]\n",
    "        next_week_data = weekly_df.iloc[i + 1]\n",
    "        for j in range(7):\n",
    "            daily_date = week_data.name + timedelta(days=j)\n",
    "            daily_values = week_data + (next_week_data - week_data) * (j / 7)\n",
    "            daily_values.name = daily_date\n",
    "            daily_data.append(daily_values)\n",
    "    return pd.DataFrame(daily_data)\n",
    "\n",
    "daily_df = convert_weekly_to_daily(interest_over_time_df)\n",
    "\n",
    "daily_df.reset_index(inplace=True)\n",
    "daily_df.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "daily_df = daily_df[(daily_df['date'] >= '2020-01-01') & (daily_df['date'] <= '2024-06-30')]\n",
    "\n",
    "print(daily_df)\n",
    "\n",
    "daily_df.to_csv('FETCHED_DATA/PY_GOOGLE_TRENDS_API/PY_GOOGLE_TRENDS.csv', index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## YOUTUBE | VIDEO COUNT"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "API_KEYS = ['-']\n",
    "\n",
    "keywords = ['\"Russia Ukraine War\"', '\"Israel Gaza War\"', '\"Financial Crisis\"', '\"Climate Change\"', '\"Covid-19\"']\n",
    "\n",
    "all_dates = pd.date_range(start='2020-01-01', end='2024-06-30', freq='M').to_period('M').tolist()\n",
    "all_dates = [str(date) for date in all_dates]\n",
    "\n",
    "def get_total_results(api_key, keyword, start_date, end_date):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    total_results = 0\n",
    "\n",
    "    try:\n",
    "        search_response = youtube.search().list(\n",
    "            q=keyword,\n",
    "            part='id,snippet',\n",
    "            maxResults=1,\n",
    "            publishedAfter=start_date,\n",
    "            publishedBefore=end_date\n",
    "        ).execute()\n",
    "\n",
    "        total_results = search_response['pageInfo']['totalResults']\n",
    "    except HttpError as e:\n",
    "        if e.resp.status == 403 and 'quotaExceeded' in e.content.decode():\n",
    "            print(f\"Quota exceeded for API key: {api_key}\")\n",
    "            raise\n",
    "        else:\n",
    "            print(f\"An HTTP error {e.resp.status} occurred:\\n{e.content}\")\n",
    "            total_results = 0\n",
    "\n",
    "    return total_results\n",
    "\n",
    "all_total_results = {}\n",
    "\n",
    "for i, keyword in enumerate(keywords):\n",
    "    api_key = API_KEYS[i % len(API_KEYS)]\n",
    "    total_results_data = {}\n",
    "    total_units_used = 0  \n",
    "\n",
    "    for j in range(len(all_dates) - 1):\n",
    "        start_date = all_dates[j] + '-01T00:00:00Z'\n",
    "        end_date = (pd.Period(all_dates[j]).end_time + timedelta(days=1)).strftime('%Y-%m-%dT00:00:00Z')\n",
    "\n",
    "        try:\n",
    "            total_results = get_total_results(api_key, keyword, start_date, end_date)\n",
    "            total_results_data[all_dates[j]] = total_results\n",
    "            total_units_used += 100  \n",
    "            time.sleep(2)  \n",
    "        except HttpError as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            total_results_data[all_dates[j]] = 0\n",
    "            if 'quotaExceeded' in str(e):\n",
    "                print(\"Quota exceeded, terminating process.\")\n",
    "                break  \n",
    "\n",
    "    all_total_results[keyword] = total_results_data\n",
    "    print(f\"Total units used for {keyword}: {total_units_used}\")\n",
    "\n",
    "total_results_df = pd.DataFrame(index=all_dates[:-1])\n",
    "\n",
    "for keyword in keywords:\n",
    "    total_results = []\n",
    "    total_results_data = all_total_results[keyword]\n",
    "\n",
    "    for date in all_dates[:-1]:\n",
    "        total_results.append(total_results_data[date])\n",
    "\n",
    "    total_results_df[f'{keyword}_total_results'] = total_results\n",
    "\n",
    "total_results_df.to_csv('FETCHED_DATA/YOUTUBE_API/YOUTUBE_TOTAL_RESULTS.csv', index_label='Date')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "df_monthly = pd.read_csv('FETCHED_DATA/YOUTUBE_API/YOUTUBE_TOTAL_RESULTS.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "all_dates = pd.date_range(start='2020-01-01', end='2024-06-30')\n",
    "df_daily = pd.DataFrame(index=all_dates)\n",
    "\n",
    "def distribute_monthly_value(value, year, month):\n",
    "    days_in_month = pd.Period(f'{year}-{month:02}').days_in_month\n",
    "    daily_values = [0] * days_in_month\n",
    "    \n",
    "    for i in range(value):\n",
    "        daily_values[random.randint(0, days_in_month - 1)] += 1\n",
    "    \n",
    "    return daily_values\n",
    "\n",
    "monthly_columns = df_monthly.columns\n",
    "\n",
    "for column in monthly_columns:\n",
    "    for date, value in df_monthly[column].items():\n",
    "        year, month = date.year, date.month\n",
    "        daily_values = distribute_monthly_value(value, year, month)\n",
    "        \n",
    "        start_date = datetime(year, month, 1)\n",
    "        end_date = datetime(year, month, pd.Period(f'{year}-{month:02}').days_in_month)\n",
    "        date_range = pd.date_range(start=start_date, end=end_date)\n",
    "        \n",
    "        df_daily.loc[date_range, column] = daily_values\n",
    "\n",
    "df_daily.fillna(0, inplace=True)\n",
    "\n",
    "df_daily.to_csv('FETCHED_DATA/YOUTUBE_API/YOUTUBE_DAILY_RESULTS.csv', index_label='Date')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
